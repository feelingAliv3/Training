{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Sequential, Functional and Model Subclassing\n",
    "This notebook is for training and understanding purposes only. All algorithms and credits go to pyimagesearch.com, specifically https://www.pyimagesearch.com/2019/10/28/3-ways-to-create-a-keras-model-with-tensorflow-2-0-sequential-functional-and-model-subclassing/and Adrian Rosebrock (A wonderful source and inspiration for Computer Vision and Deep Learning)\n",
    "\n",
    "As this notebook is for training and understanding purposes, rather than downloading the source code right away. The code will be typed in order to build \"muscle-memory\". Author-readable comments will appear from time to time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With keras, there are three ways to build model. They are \n",
    "<br>\n",
    "1. **Sequential** is the simplest method to build a model - as its name, you add layer after layer. However, it lacks the ability to build complex model (i.e. share output from two layers, concatenating two layers, multiple inputs etc) Ex. LeNet\n",
    "<br>\n",
    "2. **Functional** allows exactly such an an application - multiple inputs and multiple outputs, complex architecture. Ex. ResNet\n",
    "<br>\n",
    "3. **Model Subclassing** allows even more flexibility, specifically allowing custom forward pass path (i.e. forward pass that skips layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallownet_sequential(width, height, depth, classes):\n",
    "    # initialize the model along with the input shape to be \"channels last\" ordering\n",
    "    # can also have the model built in the sequential bracket\n",
    "    model = Sequential()\n",
    "    inputShape = (height, width, depth)\n",
    "    \n",
    "    # define the first (and only) CONV => RELU layer\n",
    "    model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=inputShape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    # softmax classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    # return the constructed network architecture\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minigooglenet_function(width, height, depth, classes):\n",
    "    def conv_module(x, K, kX, kY, stride, chanDim, padding=\"same\"):\n",
    "        # the input x here refers to previous layer and is added as input as below\n",
    "        # K = no. of filter\n",
    "        # kX, kY = shape of filter\n",
    "        # stride = how many pixels to move the filter\n",
    "        # chanDim is needed to know which axis to normalize (for image typically accross the batch (based on backend))\n",
    "        # padding = \"same\" allows to keep the input dimension as output dimension\n",
    "        x = Conv2D(K, (kX, kY), strides=stride, padding=padding)(x)\n",
    "        x = BatchNormalization(axis=chanDim)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def inception_module(x, numK1x1, numK3x3, chanDim):\n",
    "        # the input x here refers to previous layer and is added as input as below\n",
    "        # numK1x1 = number of 1x1 filter\n",
    "        # numK1x1 = number of 3x3 filter\n",
    "        conv_1x1 = conv_module(x, numK1x1, 1, 1, (1,1), chanDim)\n",
    "        conv_3x3 = conv_module(x, numK3x3, 3, 3, (1,1), chanDim)\n",
    "        x = concatenate([conv_1x1, conv_3x3], axis=chanDim)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def downsample_module(x, K, chanDim):\n",
    "        # padding = \"valid\" with (2,2) stride allows to downsize the input dimension\n",
    "        conv_3x3 = conv_module(x, K, 3, 3, (2,2), chanDim, padding=\"valid\")\n",
    "        pool = MaxPooling2D((3,3), strides=(2, 2))(x)\n",
    "        x = concatenate([conv_3x3, pool], axis=chanDim)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # once we have defined the function, we need to start define the input layer\n",
    "    # we need to first define an input layer, then we can put the input layer into\n",
    "    \n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1    \n",
    "    inputs = Input(shape = inputShape)\n",
    "    \n",
    "    x = conv_module(inputs, 96, 3, 3, (1,1), chanDim)\n",
    "    x = inception_module(x, 32, 32, chanDim)\n",
    "    x = inception_module(x, 32, 32, chanDim)\n",
    "    x = downsample_module(x, 80, chanDim)\n",
    "    \n",
    "    x = inception_module(x, 112, 48, chanDim)\n",
    "    x = inception_module(x, 96, 64, chanDim)\n",
    "    x = inception_module(x, 80, 80, chanDim)\n",
    "    x = inception_module(x, 48, 96, chanDim)\n",
    "    x = downsample_module(x, 96, chanDim)\n",
    "    \n",
    "    x = inception_module(x, 176, 160, chanDim)\n",
    "    x = inception_module(x, 176, 160, chanDim)\n",
    "    x = AveragePooling2D((7, 7))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    # softmax classifier\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(classes)(x)\n",
    "    x = Activation(\"softmax\")(x)\n",
    "    # create the model\n",
    "    model = Model(inputs, x, name=\"minigooglenet\")\n",
    "    # return the constructed network architecture\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a class and inheritance method to build complex model\n",
    "class MiniVGGNetModel(Model):\n",
    "    def __init__(self, classes, chanDim=-1):\n",
    "        super(MiniVGGNetModel, self).__init__()\n",
    "        \n",
    "        self.conv1A = Conv2D(32,(3,3), padding=\"same\")\n",
    "        self.act1A = Activation(\"relu\")\n",
    "        self.bn1A = BatchNormalization(axis=chanDim)\n",
    "        self.conv1B = Conv2D(32,(3,3), padding=\"same\")\n",
    "        self.act1B = Activation(\"relu\")\n",
    "        self.bn1B = BatchNormalization(axis=chanDim)\n",
    "        self.pool1 = MaxPooling2D(pool_size=(2, 2))\n",
    "        self.conv2A = Conv2D(32, (3, 3), padding=\"same\")\n",
    "        self.act2A = Activation(\"relu\")\n",
    "        self.bn2A = BatchNormalization(axis=chanDim)\n",
    "        self.conv2B = Conv2D(32, (3, 3), padding=\"same\")\n",
    "        self.act2B = Activation(\"relu\")\n",
    "        self.bn2B = BatchNormalization(axis=chanDim)\n",
    "        self.pool2 = MaxPooling2D(pool_size=(2, 2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense3 = Dense(512)\n",
    "        self.act3 = Activation(\"relu\")\n",
    "        self.bn3 = BatchNormalization()\n",
    "        self.do3 = Dropout(0.5)\n",
    "        self.dense4 = Dense(classes)\n",
    "        self.softmax = Activation(\"softmax\")\n",
    "        \n",
    "    # important function for forward pass\n",
    "    # uses functional API method\n",
    "    def call(self, inputs):\n",
    "        # build the first (CONV => RELU) * 2 => POOL layer set\n",
    "        x = self.conv1A(inputs)\n",
    "        x = self.act1A(x)\n",
    "        x = self.bn1A(x)\n",
    "        x = self.conv1B(x)\n",
    "        x = self.act1B(x)\n",
    "        x = self.bn1B(x)\n",
    "        x = self.pool1(x)\n",
    "        # build the second (CONV => RELU) * 2 => POOL layer set\n",
    "        x = self.conv2A(x)\n",
    "        x = self.act2A(x)\n",
    "        x = self.bn2A(x)\n",
    "        x = self.conv2B(x)\n",
    "        x = self.act2B(x)\n",
    "        x = self.bn2B(x)\n",
    "        x = self.pool2(x)\n",
    "        # build our FC layer set\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.do3(x)\n",
    "        # build the softmax classifier\n",
    "        x = self.dense4(x)\n",
    "        x = self.softmax(x)\n",
    "        # return the constructed model\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "LR = 1e-2\n",
    "BS = 64\n",
    "\n",
    "# Instantiate model (assume for MNIST dataset)\n",
    "model = shallownet_sequential(28,28,1,10)\n",
    "# Introduce an optimizer with no weights decay\n",
    "opt = Adam(lr=INIT_LR)\n",
    "# compile everything\n",
    "model.compile(optimizer = opt, loss = categorical_crossentropy, metrics = [\"acc\"])\n",
    "# if building with gradientTape, dont need to fit, since you will have to define the actual forward pass step function\n",
    "# in the stepping function, you would have to calculate the loss function, how to apply the gradient in each epoch.\n",
    "# i.e. for num in epoch: model = forwardpass(input_x), loss = , gradient = , apply gradient =\n",
    "# Won't be able to validate data?\n",
    "model.fit(trainX, trainY, validation_data = (testX, testY), epochs=EPOCHS, batch_size=BS) # any other custom configuration\n",
    "# if generator object\n",
    "model.fit_generator ()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

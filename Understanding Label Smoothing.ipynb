{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Smoothing\n",
    "This notebook is for training and understanding purposes only. All algorithms and credits go to pyimagesearch.com, specifically https://www.pyimagesearch.com/2019/12/30/label-smoothing-with-keras-tensorflow-and-deep-learning/ and Adrian Rosebrock (A wonderful source and inspiration for Computer Vision and Deep Learning)\n",
    "\n",
    "As this notebook is for training and understanding purposes, rather than downloading the source code right away. The code will be typed in order to build \"muscle-memory\". Author-readable comments will appear from time to time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is label smoothing?\n",
    "When we trained a model, we hope that the model generalizes well. There are regularization techniques such as neuron dropout, data augmentation, L2 weight decay etc to achieve such puposes. And label smoothing is another technique to do so. <br>\n",
    "<br>\n",
    "Put simply, label smoothing is a technique to change \"hard-coded\" outcome/label into \"soft-coded\" outcome/label. The classical example of a \"hard-coded\" label is the one-hot encoding where we input sparse vectors of 0s and 1s only (i.e. [0, 0, 0, 1, 0, 0 ] as our target. In one-hot encoding, we are effectively saying, we are 100% confident that it is indeed the label we specified. What Label smoothing involve transitioning from [0.01, 0.01, 0.01, 0.96, 0.01].<br>\n",
    "<br>\n",
    "This effectively change our loss function (remember $y log(p) + (1-y) log (1-p)$), now that y is no longer 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "# import the necessary packages\n",
    "from pyimagesearch.learning_rate_schedulers import PolynomialDecay\n",
    "from pyimagesearch.minigooglenet import MiniGoogLeNet\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "def smooth_labels(labels, factor=0.1):\n",
    "    # smooth the labels\n",
    "    labels *= (1 - factor)\n",
    "    labels += (factor / labels.shape[1])\n",
    "    # returned the smoothed labels\n",
    "    return labels\n",
    "\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-s\", \"--smoothing\", type=float, default=0.1, help=\"amount of label smoothing to be applied\")\n",
    "# ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\", help=\"path to output plot file\")\n",
    "# args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading CIFAR-10 data...\n",
      "[INFO] smoothing amount: 0.1\n",
      "[INFO] before smoothing: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[INFO] after smoothing: [0.01 0.01 0.01 0.01 0.01 0.01 0.91 0.01 0.01 0.01]\n",
      "[INFO] compiling model...\n",
      "[INFO] training network...\n",
      "Epoch 1/10\n",
      "781/781 [==============================] - 1492s 2s/step - loss: 1.7095 - accuracy: 0.4451 - val_loss: 1.3002 - val_accuracy: 0.5410\n",
      "Epoch 2/10\n",
      "781/781 [==============================] - 1468s 2s/step - loss: 1.3960 - accuracy: 0.6026 - val_loss: 1.2259 - val_accuracy: 0.5737\n",
      "Epoch 3/10\n",
      "781/781 [==============================] - 1466s 2s/step - loss: 1.2725 - accuracy: 0.6670 - val_loss: 0.9707 - val_accuracy: 0.6692\n",
      "Epoch 4/10\n",
      "781/781 [==============================] - 1468s 2s/step - loss: 1.1909 - accuracy: 0.7113 - val_loss: 0.8823 - val_accuracy: 0.6990\n",
      "Epoch 5/10\n",
      "781/781 [==============================] - 1467s 2s/step - loss: 1.1306 - accuracy: 0.7405 - val_loss: 0.8399 - val_accuracy: 0.7245\n",
      "Epoch 6/10\n",
      "781/781 [==============================] - 1465s 2s/step - loss: 1.0863 - accuracy: 0.7613 - val_loss: 0.7153 - val_accuracy: 0.7722\n",
      "Epoch 7/10\n",
      "781/781 [==============================] - 1465s 2s/step - loss: 1.0522 - accuracy: 0.7809 - val_loss: 0.6589 - val_accuracy: 0.7855\n",
      "Epoch 8/10\n",
      "781/781 [==============================] - 1474s 2s/step - loss: 1.0228 - accuracy: 0.7929 - val_loss: 0.6456 - val_accuracy: 0.7974\n",
      "Epoch 9/10\n",
      "781/781 [==============================] - 1466s 2s/step - loss: 0.9984 - accuracy: 0.8057 - val_loss: 0.6285 - val_accuracy: 0.8028\n",
      "Epoch 10/10\n",
      "781/781 [==============================] - 1466s 2s/step - loss: 0.9783 - accuracy: 0.8154 - val_loss: 0.6070 - val_accuracy: 0.8145\n"
     ]
    }
   ],
   "source": [
    "# define the total number of epochs to train for, initial learning rate, and batch size\n",
    "NUM_EPOCHS = 10\n",
    "INIT_LR = 5e-3\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# initialize the label names for the CIFAR-10 dataset\n",
    "labelNames = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "# load the training and testing data, converting the images from integers to floats\n",
    "print(\"[INFO] loading CIFAR-10 data...\")\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "trainX = trainX.astype(\"float\")\n",
    "testX = testX.astype(\"float\")\n",
    "\n",
    "# apply mean subtraction to the data\n",
    "mean = np.mean(trainX, axis=0)\n",
    "trainX -= mean\n",
    "testX -= mean\n",
    "\n",
    "# convert the labels from integers to vectors, converting the data type to floats so we can apply label smoothing\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)\n",
    "trainY = trainY.astype(\"float\")\n",
    "testY = testY.astype(\"float\")\n",
    "\n",
    "# apply label smoothing to the *training labels only*\n",
    "# print(\"[INFO] smoothing amount: {}\".format(args[\"smoothing\"]))\n",
    "print(\"[INFO] smoothing amount: {}\".format(0.1))\n",
    "print(\"[INFO] before smoothing: {}\".format(trainY[0]))\n",
    "trainY = smooth_labels(trainY, 0.1)\n",
    "print(\"[INFO] after smoothing: {}\".format(trainY[0]))\n",
    "\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "# a powerful tool from keras to augment more data (change in rotation, transformation, etc)\n",
    "aug = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "# construct the learning rate scheduler callback\n",
    "# this is a callback function for change in learning rate, instead of fix learning rate\n",
    "schedule = PolynomialDecay(maxEpochs=NUM_EPOCHS, initAlpha=INIT_LR, power=1.0)\n",
    "callbacks = [LearningRateScheduler(schedule)]\n",
    "\n",
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(lr=INIT_LR, momentum=0.9)\n",
    "model = MiniGoogLeNet.build(width=32, height=32, depth=3, classes=10)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit_generator( aug.flow(trainX, trainY, batch_size=BATCH_SIZE), validation_data=(testX, testY), steps_per_epoch=(len(trainX) // BATCH_SIZE), epochs=NUM_EPOCHS, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method is to initialize the loss function with label smoothing directly.\n",
    "<br>\n",
    "i.e.\n",
    "loss = CategoricalCrossentropy(label_smoothing=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
